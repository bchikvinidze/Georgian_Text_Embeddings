{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.25.2\n",
      "  Downloading numpy-1.25.2-cp311-cp311-win_amd64.whl.metadata (5.7 kB)\n",
      "Downloading numpy-1.25.2-cp311-cp311-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.5 MB 487.6 kB/s eta 0:00:32\n",
      "   ---------------------------------------- 0.1/15.5 MB 744.7 kB/s eta 0:00:21\n",
      "   ---------------------------------------- 0.1/15.5 MB 847.9 kB/s eta 0:00:19\n",
      "    --------------------------------------- 0.3/15.5 MB 1.2 MB/s eta 0:00:13\n",
      "    --------------------------------------- 0.3/15.5 MB 1.2 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.5/15.5 MB 1.5 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.7/15.5 MB 1.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.8/15.5 MB 1.9 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.0/15.5 MB 2.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.2/15.5 MB 2.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.3/15.5 MB 2.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.5/15.5 MB 2.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.6/15.5 MB 2.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.8/15.5 MB 2.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.0/15.5 MB 2.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.1/15.5 MB 2.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.1/15.5 MB 2.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.3/15.5 MB 2.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.3/15.5 MB 2.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.3/15.5 MB 2.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.3/15.5 MB 2.3 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.6/15.5 MB 2.3 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.9/15.5 MB 2.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 3.1/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.2/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.3/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.4/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.6/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.7/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.9/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.0/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.1/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.2/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.4/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.5/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.6/15.5 MB 2.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.7/15.5 MB 2.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.9/15.5 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.0/15.5 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.2/15.5 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.4/15.5 MB 2.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.6/15.5 MB 2.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.7/15.5 MB 2.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.8/15.5 MB 2.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 6.0/15.5 MB 2.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 6.1/15.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.3/15.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.4/15.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.5/15.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.6/15.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.7/15.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.8/15.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.9/15.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.1/15.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.2/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.4/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.5/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.7/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.8/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.0/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.1/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.3/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.3/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.3/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.4/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.6/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.7/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.8/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.0/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.1/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.3/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 9.4/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 9.6/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.7/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.8/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.9/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 10.0/15.5 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.2/15.5 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.4/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.6/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.7/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.8/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.9/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.0/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.2/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.3/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.5/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.6/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.8/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.0/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.1/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.2/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.4/15.5 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 12.5/15.5 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 12.6/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.8/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.9/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.0/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.3/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.5/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.7/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.8/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.0/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.1/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.3/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.4/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.5/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.8/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.9/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.5 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.5 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.3/15.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.4/15.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "Successfully installed numpy-1.25.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers~=2.2.2 in .\\.conda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (4.41.2)\n",
      "Requirement already satisfied: tqdm in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (2.3.1)\n",
      "Requirement already satisfied: torchvision in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (0.18.1)\n",
      "Requirement already satisfied: numpy in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (1.5.0)\n",
      "Requirement already satisfied: scipy in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (1.13.1)\n",
      "Requirement already satisfied: nltk in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (0.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in .\\.conda\\lib\\site-packages (from sentence_transformers~=2.2.2) (0.23.4)\n",
      "Requirement already satisfied: filelock in .\\.conda\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (3.15.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\.conda\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bubu.chikvinidze\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\.conda\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (6.0.1)\n",
      "Requirement already satisfied: requests in .\\.conda\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\.conda\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in .\\.conda\\lib\\site-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (1.12.1)\n",
      "Requirement already satisfied: networkx in .\\.conda\\lib\\site-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (3.3)\n",
      "Requirement already satisfied: jinja2 in .\\.conda\\lib\\site-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in .\\.conda\\lib\\site-packages (from torch>=1.6.0->sentence_transformers~=2.2.2) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bubu.chikvinidze\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence_transformers~=2.2.2) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers~=2.2.2) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in .\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers~=2.2.2) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in .\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers~=2.2.2) (0.4.3)\n",
      "Requirement already satisfied: click in .\\.conda\\lib\\site-packages (from nltk->sentence_transformers~=2.2.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in .\\.conda\\lib\\site-packages (from nltk->sentence_transformers~=2.2.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in .\\.conda\\lib\\site-packages (from scikit-learn->sentence_transformers~=2.2.2) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in .\\.conda\\lib\\site-packages (from torchvision->sentence_transformers~=2.2.2) (10.3.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in .\\.conda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.6.0->sentence_transformers~=2.2.2) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in .\\.conda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.6.0->sentence_transformers~=2.2.2) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.conda\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers~=2.2.2) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers~=2.2.2) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in .\\.conda\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers~=2.2.2) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers~=2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mteb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mteb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmteb\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mteb'"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import mteb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asttokens==2.4.1\n",
      "certifi==2024.6.2\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "comm==0.2.1\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "executing==2.0.1\n",
      "filelock==3.15.3\n",
      "fsspec==2024.6.0\n",
      "huggingface-hub==0.23.4\n",
      "idna==3.7\n",
      "intel-openmp==2021.4.0\n",
      "ipykernel==6.29.0\n",
      "ipython==8.20.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.4\n",
      "joblib==1.4.2\n",
      "jupyter_client==8.6.0\n",
      "jupyter_core==5.7.1\n",
      "MarkupSafe==2.1.5\n",
      "matplotlib-inline==0.1.6\n",
      "mkl==2021.4.0\n",
      "mpmath==1.3.0\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.3\n",
      "nltk==3.8.1\n",
      "numpy==1.25.2\n",
      "packaging==23.2\n",
      "parso==0.8.3\n",
      "pillow==10.3.0\n",
      "platformdirs==4.1.0\n",
      "prompt-toolkit==3.0.43\n",
      "psutil==5.9.8\n",
      "pure-eval==0.2.2\n",
      "Pygments==2.17.2\n",
      "python-dateutil==2.8.2\n",
      "pywin32==306\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.2\n",
      "regex==2024.5.15\n",
      "requests==2.32.3\n",
      "safetensors==0.4.3\n",
      "scikit-learn==1.5.0\n",
      "scipy==1.13.1\n",
      "sentence-transformers==2.2.2\n",
      "sentencepiece==0.2.0\n",
      "six==1.16.0\n",
      "stack-data==0.6.3\n",
      "sympy==1.12.1\n",
      "tbb==2021.13.0\n",
      "threadpoolctl==3.5.0\n",
      "tokenizers==0.19.1\n",
      "torch==2.3.1\n",
      "torchvision==0.18.1\n",
      "tornado==6.4\n",
      "tqdm==4.66.4\n",
      "traitlets==5.14.1\n",
      "transformers==4.41.2\n",
      "typing_extensions==4.12.2\n",
      "urllib3==2.2.2\n",
      "wcwidth==0.2.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining functions:\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "input_texts = ['query: how much protein should a female eat',\n",
    "               'query: 南瓜的家常做法',\n",
    "               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "               \"passage: 1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右,放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models:\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "model_with_e5 = AutoModel.from_pretrained('intfloat/multilingual-e5-small')\n",
    "model_with_sentence_transformer = SentenceTransformer('intfloat/multilingual-e5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,     41,   1294,     12,   3642,   5045,  21308,   5608,     10,\n",
       "         117776,  73203,      2,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1],\n",
       "        [     0,     41,   1294,     12,      6,   4617,  39613,     43,   1433,\n",
       "           6856, 115371,      2,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1],\n",
       "        [     0,  46692,     12,   1301,     10,   4537,  17997,   2256,      4,\n",
       "             70,   7915,    441,     25,      7,  83080,  64209,    674,    111,\n",
       "          21308,    100,  24793,     10,   4188,    953,     47,   2358,     83,\n",
       "           7621,  16190,      7,    117,   5155,      5,   4966,      4,    237,\n",
       "            398,    831,   1957,   1295,    903, 116287,      4,    398,     25,\n",
       "           1181,   3871,     47,  51312,    450,   2174,    398,     25,    107,\n",
       "          41206,    214,    707,  23189,    100,     10, 179365,      5,  38679,\n",
       "           1810,     70, 116287,  35064,     47,   1957,   3642,   5045,  21308,\n",
       "            398,   5608,    186, 118992,  12638,   5155,      5,      2,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1],\n",
       "        [     0,  46692,     12,    615,   7318,  54107,   4617,  39613,  59580,\n",
       "              6, 105336,     12,  88232,   4617,  39613,   6193,   3294,      6,\n",
       "          17619,   8794,     12, 243853,     37, 152890,     37,   3515,  25407,\n",
       "             37,  60793,   8539,      6, 115371,     12,  74559,   4617,  39613,\n",
       "           1173,  27322,  27080,  27080,     43, 101523,   1677,  51655, 191224,\n",
       "          10103,      4,   1173, 244307,   1344, 146698,   1677,      3,  82258,\n",
       "          67903,   2803,  36332,  59580,    132,   3029,  67903,  10569,  10930,\n",
       "            887,   1173,  27322,  34159,   7847,   2803,  36332,  59580,     16,\n",
       "         101216, 138300,  82264,  11802,   5853,   6858,      4,   2283, 243853,\n",
       "           2603, 248002,   1040, 184005,    201,     37,   2283,   4617,  39613,\n",
       "          59580,  16390,  21233,  54107,    684,  22801,  15369,      4,   5853,\n",
       "         152890,     37,  18318,   3515,  25407,    264,  60793,   8539,  17619,\n",
       "           7835,   1040, 138300,    787,   8849, 243853,  54107,   4617,  39613,\n",
       "              6, 105336,     12,   4617,  39613,    418,   5344,      6,  17619,\n",
       "           8794,     12,   8849, 243853,     37, 169622,  27415,     37, 249924,\n",
       "         244053,   6858,     37, 152890,      6, 115371,     12,  74559,   1726,\n",
       "           4617,  39613,   1677,  10103,      4,   7847,   2803,   5143,  82258,\n",
       "           6858, 138300,   1019,   2803,  11802,   1826,      4,   1726, 169622,\n",
       "          27415, 153207,  20265,   8849, 101216,  20265,   8849,   1826,      4,\n",
       "           1726,   4617,  39613,   5143, 153207,      4,  21233,  54107,    201,\n",
       "             37,    213,  21233,  54107,  62736,      4,   1441,    562,   1898,\n",
       "            955,   7818, 138300,   2008,   3490,   1553,      4,   1273,   7402,\n",
       "          30583,    190,     37, 153207, 152890,      4,  54107, 244285,    305,\n",
       "             37,   4617,  39613,  86579,  60772,    264, 202992,    274,   8856,\n",
       "              4,  20297,  15900,   5505,    361,     37,  79007,   2283,   8849,\n",
       "         243853,      4,  30060,   1040, 138300,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2526, -0.0612, -0.2972,  ...,  0.1744,  0.0097,  0.1362],\n",
       "         [ 0.1764, -0.3090, -0.2646,  ...,  0.1445,  0.1458,  0.2989],\n",
       "         [ 0.1566, -0.3100, -0.2594,  ...,  0.1459,  0.1691,  0.2915],\n",
       "         ...,\n",
       "         [ 0.3977, -0.0538, -0.2153,  ...,  0.1540,  0.0514,  0.1949],\n",
       "         [ 0.3768, -0.0546, -0.2243,  ...,  0.1711,  0.0454,  0.1951],\n",
       "         [ 0.3611, -0.0538, -0.2445,  ...,  0.1663,  0.0781,  0.1949]],\n",
       "\n",
       "        [[ 0.2138, -0.0392, -0.2063,  ...,  0.3219,  0.1744, -0.0456],\n",
       "         [ 0.3034, -0.0494, -0.2561,  ...,  0.3990,  0.3571,  0.0787],\n",
       "         [ 0.3038, -0.0386, -0.2365,  ...,  0.3948,  0.3628,  0.0766],\n",
       "         ...,\n",
       "         [ 0.3162,  0.0336, -0.1920,  ...,  0.4149,  0.3290,  0.0169],\n",
       "         [ 0.2990,  0.0231, -0.2115,  ...,  0.4166,  0.3110,  0.0275],\n",
       "         [ 0.2938,  0.0137, -0.2153,  ...,  0.4144,  0.3266,  0.0344]],\n",
       "\n",
       "        [[ 0.1229, -0.0880, -0.1926,  ...,  0.3122,  0.0581,  0.1751],\n",
       "         [-0.3849, -0.1974, -0.2212,  ...,  0.4247,  0.1647,  0.2215],\n",
       "         [-0.4142, -0.1790, -0.2299,  ...,  0.4018,  0.1580,  0.2072],\n",
       "         ...,\n",
       "         [ 0.1387, -0.2354, -0.2185,  ...,  0.3728,  0.1863,  0.3262],\n",
       "         [ 0.1517, -0.2366, -0.2077,  ...,  0.3705,  0.1925,  0.3240],\n",
       "         [ 0.1599, -0.2282, -0.2142,  ...,  0.3700,  0.1969,  0.3258]],\n",
       "\n",
       "        [[ 0.2120, -0.0762, -0.2165,  ...,  0.2805,  0.1867,  0.0332],\n",
       "         [-0.1681,  0.1635, -0.0975,  ...,  0.3530,  0.3907,  0.0248],\n",
       "         [-0.2213,  0.2002, -0.0706,  ...,  0.2957,  0.3312,  0.0033],\n",
       "         ...,\n",
       "         [ 0.2827, -0.1254, -0.1204,  ...,  0.3513,  0.1236,  0.0370],\n",
       "         [ 0.0897,  0.0507, -0.1345,  ...,  0.1030,  0.2939,  0.1202],\n",
       "         [ 0.2120, -0.0762, -0.2165,  ...,  0.2805,  0.1867,  0.0332]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.0595,  0.0154,  0.0380,  ..., -0.1412, -0.1070,  0.1162],\n",
       "        [ 0.0420, -0.0110,  0.0205,  ..., -0.0977, -0.0996,  0.0852],\n",
       "        [ 0.0620,  0.0223,  0.0318,  ..., -0.1352, -0.1212,  0.1026],\n",
       "        [ 0.0394,  0.0106,  0.0033,  ..., -0.1030, -0.1034,  0.0758]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model_with_e5(**batch_dict)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2113, -0.1942, -0.2871,  ...,  0.1289,  0.1541,  0.2488],\n",
       "        [ 0.2518, -0.0720, -0.1733,  ...,  0.3655,  0.2942,  0.0529],\n",
       "        [-0.1302, -0.2040, -0.2224,  ...,  0.3290,  0.2358,  0.2935],\n",
       "        [ 0.0924, -0.0120, -0.1785,  ...,  0.2917,  0.3686,  0.0910]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0422, -0.0388, -0.0573,  ...,  0.0257,  0.0308,  0.0497],\n",
       "        [ 0.0534, -0.0153, -0.0367,  ...,  0.0775,  0.0623,  0.0112],\n",
       "        [-0.0258, -0.0404, -0.0440,  ...,  0.0651,  0.0467,  0.0581],\n",
       "        [ 0.0194, -0.0025, -0.0375,  ...,  0.0613,  0.0775,  0.0191]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[92.72809600830078, 75.96249389648438], [74.38050842285156, 90.60895538330078]]\n"
     ]
    }
   ],
   "source": [
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[92.72811126708984, 75.96250915527344], [74.3805160522461, 90.60896301269531]]\n"
     ]
    }
   ],
   "source": [
    "embeddings_with_sentence_transformer = model_with_sentence_transformer.encode(input_texts, normalize_embeddings=True)\n",
    "scores_with_sentence_transformer = (embeddings_with_sentence_transformer[:2] @ embeddings_with_sentence_transformer[2:].T) * 100\n",
    "print(scores_with_sentence_transformer.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0421759 , -0.03877136, -0.05730906, ...,  0.025732  ,\n",
       "         0.03076708,  0.04966562],\n",
       "       [ 0.05337086, -0.01525041, -0.03673498, ...,  0.07745242,\n",
       "         0.06234608,  0.01122016],\n",
       "       [-0.02577215, -0.04037194, -0.04400872, ...,  0.06510442,\n",
       "         0.04665779,  0.05807815],\n",
       "       [ 0.01940993, -0.00252936, -0.03752143, ...,  0.06130136,\n",
       "         0.07746346,  0.01912975]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_with_sentence_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some improvement observed, although negligible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
